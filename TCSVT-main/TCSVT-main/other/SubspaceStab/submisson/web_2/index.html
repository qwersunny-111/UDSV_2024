<!doctype html>
<html>
  <head>
    <title>CSE555 Project Report</title>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'],
      ['\\(','\\)']]}});
    </script>
    <script type="text/javascript"
	    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
  </head>
  <body>
    <h1>CSE 555 Final Project: Subspace Video Stabilization</h1>
    <h2>Hang Yan (yanhang@wustl.edu)</h2>
    <p>Implemented in C++. Code can be downloaded
      from <a href="https://github.com/higerra/SubspaceStab.git">https://github.com/higerra/SubspaceStab.git</a></p>

    <h1>Introduction</h1>
    <p>In this project I implemented the following two papers:</p>
    <p><a href="http://web.cecs.pdx.edu/~fliu/papers/tog2010.pdf">Subspace
	Video Stabilization
    </a></p>
    <p><a href="http://gvv.mpi-inf.mpg.de/teaching/gvv_seminar_2012/papers/Content-Preserving%20Warps%20for%203D%20Video%20Stabilization.pdf">Content
	Preserving Warps for 3D Video Stabilization</a></p>
    
    The problem tackled in this project is to take an input video
    captured by a hand held camera, produce an output video that is
    more stable. One example is shown below:</p>
<table>
  <tr>
    <td>
      <iframe width="560" height="315"
      src="https://www.youtube.com/embed/It9Yuh0mTrs" frameborder="0"
      allowfullscreen></iframe>
    </td>
    <td>
      <iframe width="560" height="315"
      src="https://www.youtube.com/embed/ztwhL0oixws" frameborder="0"
      allowfullscreen></iframe>
    </td>
  </tr>
</table>


<h2>Related Work</h2>
<p>There are numerous works on video stabilization. In this section I
  briefly summerize parts of them.</p>
<p>Video stabilization algorithms can be roughly categroized into two
  branches: 2D methods and 3D methods. The difference is whether 3D
  information is computed and used.</p>
<p>The method proposed by Liu et al[1] is a good example of the second
  category. They estimate the camera pose and sparse 3D points by
  Structure from Motion. Camera path is then smoothed in 3D space. The
  3D methods provide physically accurate way to smooth the camera
  path. It is also robust to dynamic content, since modern SfM systems
  can robustly identify dynamic content as outliers. However,
  restricted by SfM, the 3D method can not apply to video with small
  baseline. Also, the running time can easily achieve to hours for
  long video sequences, mainly caused for SfM.</p>
<p>2D methods do not estimate 3D geometry of the scene. The core
  algorithm of this project belongs to this category. The algorithm
  works by first extracing and tracking feature points across the
  video, smooth feature trajetories and re-render the video according
  to new feature locations.</p>

<h2>3. Algorithm</h2>
<p>In this project, I implement the whole algorithm from the two
  papers. In this section, I briefly summarize the algorithms</p>
<h3>3.1 Feature tracking and smoothing</h3>
<p>The algorithm starts by extracting and tracking feature points
  across the video. I used Harris corner points and LK optical flow
  algorithm inside OpenCV library.</p>
<p>The output of this step is a $2m\times N$ trajetory matrix, where
  $m$ is the number of all feature tracks and $N$ is the number of
  frames:</p> $ \begin{equation} M_{2m\times N}=\begin{bmatrix} x_0^0
  & x_0^1 & x_0^2 & ... & x_0^n\\ y_0^0 & y_0^1 & y_0^2 & ... &
  y_0^n\\ x_1^0 & x_1^1 & x_1^2 & ... & x_1^n\\ y_1^0 & y_1^1 & y_1^2
  & ... & y_1^n\\ ...\\ x_m^0 & x_m^1 & x_m^2 & ... & x_m^n\\ y_m^0 &
  y_m^1 & y_m^2 & ... & y_m^n \end{bmatrix} \end{equation} $
<p>Notice that since not all tracks exist across the whole video, the
  above matrix is highly incomplete. A typical pattern of the matrix
  is shown below (figure from the original paper). Notice that the
  typically $m>>N$, the figure is shrinked in the vertical direction
  to make it a square.</p>  <image height="200px"
  src="images/pattern.png"/>

<p>To stabilize video, we want to smooth the trajetory of each
  feature. However, given the incomplete nature of the trajetory
  matrix, smoothing them individually will cause artifacts, due to the
  different temporal window of each smoothing.</p>

<h3>3.2 Subspace Constraint</h3>
<p>In this section, we assume that the scene is static. To avoid
  artifacts caused by individual smoothing, we need to enforce the 3D
  constraint to the location of smoothed trajetory. There is a well
  known fact in computer vision: under the orthogonal projection, the
  trajetories of static scene points lie on a low rank
  subspace. Though this constraint does not hold true to commonly used
  pinhole camera model, we can still approximate this low rank
  property locally in a small temporal window. We can then factorize
  the matrix $M$ into a coefficience matrix $C$ and a base matrix
  $E$:</p> $ \begin{equation} M=\begin{bmatrix}{ } \\ C\\{
  }\end{bmatrix}\begin{bmatrix}{} & E & {}\end{bmatrix} \end{equation}
  $
<p>where $C$ is a $2m\times r$ matrix and $E$ is a $r\times N$
  matrix. In the paper, $r$ is set to 9. Matrix $E$ is then regarded
  as the eigen trajetories. We can then perform the smoothing to eigen
  trajetories and get the smoothed tragetory in original space by
  multiplying back the $C$ matrix. By doing this, we enfore the 3D
  rigid constraint to the smoothing of each trajetory.</p>

<p>Since this decomposition is only true locally, the algorithm
  factorize the whole $M$ matrix in a moving fashion. Given a local
  temporal window of $k$ frames, we first construct a submatrix $M'$
  of $M$ by picking trajetories that are complete inside the
  window. We then compute SVD decomposition of $M'$. We pick the $r$
  rows of $U$ matrix and $r$ columns of $V$ matrix corresponding to
  $r$ largest singular value as local $C$ and $E$,
  respectively. Notice that the square root of $r$ singular values are
  multiplied into $U$ and $V$. We now have the coeffcience of these
  complete tracks and eigen trajetories inside this local window. The
  algorithm then moves the window forward by $\theta$ frames and
  factorize the new temporal window. In my implementaion, I choose
  $k=50$ and $\theta =5$</p>
<p>One observation of the moving factorization is that the $M'$
  matrices of two adjacent window are highly overlaped. The author
  proposed a high quality closed form approximation to the
  factorization of the second window, given the factorization of the
  first window. The mathematical details are given in the paper.</p>
<p>After factorization, we smooth the eigen trajetories (rows of $E$
  matrix) by a gaussian kernel of size 50 and compute the new location
  of each feature points by $M_{smooth} = C\times E_{smooth}$. Notice
  that $M_{smooth}$ is a complete matrix. We simply ignore the entries
  that are missing in the original $M$ matrix.</p>
<p>The above derivation comes from the assumption that all feature
  trajetories are from static objects and does not hold true for
  dynamic content. The algorithm identifies the dynamic tragetories by
  epipolar constraints and remove then from factorization.</p>

<h3>Content Preserving Image Warping</h3>
<p>Another key component of the system is image warping. For each
  frame, we want to warp this frame so that its feature locations $P$
  agree with the smoothed location $P'$. The second paper I
  implemented tackled this problem. The basic idea of warping is to
  establish a sparse mesh on the image, and deform the mesh to the
  target image, so that the locations of corresponding feature points
  agree with each other.</p>
<p>Let $V_i$ be the image coordinate of each vertex of the mesh. We
  want to solve for the location of each vertex $V_i'$ on the output
  frame. The main constraint here is that the deformed location of the
  feature points should be close to the target locations. For each
  feature points $P_i$, we can represent its coordinate by the linear
  combination of four mesh vertices $V_i^1,V_i^2,V_i^3,V_i^4$
  surrounding it (bilinear interpolation):</p> $ \begin{equation}
  P_i=\sum_{j=1}^4{w_jV_i^j} \end{equation} $
<p>So the constraint can be written as:</p> $ \begin{equation} E_V =
\sum_{i=1}^m{(\sum_{j=1}^4{w_i^j V_i^j - P_i')^2}} \end{equation} $
<p>Since we only have limited number of feature correspondence, large
  portion of the mesh will be unconstrainted. A regularization term is
  added to constraint the remaining vertices. In short, we want the
  mesh to preserve shape. For every three neighboring vertices that
  form a triangle, we want the triangle formed by the same three
  vertices in the output mesh to be the similarity transformation.</p>
  $ \begin{equation} E_S(V_1) =
  (V_1-V_2-u(V_3-V_2)-vR_{90}(V_3-V_2))^2 \end{equation} $

<p>The above optimization problem is a standard least square problem
  and can be solved efficiently by sparse QR decomposition.</p>

<h2>Experiments</h2>
<p>The three examples are taken from the paper. To make sure my
  implementation is reasonable. The first column is the input video,
  the second column is the result from my implementation, the third
  row shows the result from the original implementation by author and
  the result by 3D stabilization method. Notice that the results from
  the author use linear path planning, while I use simple path
  smoothing.</p>
<p>Due to the efficient moving factorization and closed form solution
  to the warping, the algorithm achieves around 5 fps for feature
  tracking, over 500 fps for moving factorization and over 50 fps for
  image warping with 1080p video input.</p>
<table>
  <tr>
    <td>Input video</td>
    <td>My result</td>
    <td>Author's result and 3D stabilization</td>
  </tr>
  <tr>
    <td>
      <iframe width="560" height="315"
      src="https://www.youtube.com/embed/jrz63BiI8CE" frameborder="0"
      allowfullscreen></iframe>
    </td>
    <td>
      <iframe width="560" height="315"
      src="https://www.youtube.com/embed/SRe726_9P9A" frameborder="0"
      allowfullscreen></iframe>
    </td>
    <td>
      <iframe width="560" height="315"
      src="https://www.youtube.com/embed/7GKp4ykTcOw" frameborder="0"
      allowfullscreen></iframe>
    </td>
  </tr>
  <tr>
    <td>
      <iframe width="560" height="315"
      src="https://www.youtube.com/embed/G37VVoiQNOs" frameborder="0"
      allowfullscreen></iframe>
    </td>
    <td>
      <iframe width="560" height="315"
      src="https://www.youtube.com/embed/axFhliMRYiE" frameborder="0"
      allowfullscreen></iframe>
    </td>
    <td>
      <iframe width="560" height="315"
      src="https://www.youtube.com/embed/Y4-Vbpr3XzY" frameborder="0"
      allowfullscreen></iframe>
    </td>
  </tr>
  <tr>
    <td>
      <iframe width="560" height="315"
      src="https://www.youtube.com/embed/u_dIHOD8b68" frameborder="0"
      allowfullscreen></iframe>
    </td>
    <td>
      <iframe width="560" height="315"
      src="https://www.youtube.com/embed/OKAUpC7Wic8" frameborder="0"
      allowfullscreen></iframe>
    </td>
    <td>
      <iframe width="560" height="315"
      src="https://www.youtube.com/embed/OnLQzqOzLh0" frameborder="0"
      allowfullscreen></iframe>
    </td>
  </tr>
</table>
<p>These results are successful. The video are smoothed without much
  visual artifacts. Notice that in the second experiment, there is a
  small portion of dynamic content (biker) and is handled by epipolar
  constraint.</p>
<p>Some additional examples captured by myself:</p>
<table>
  <tr>
    <td>Input Video</td>
    <td>Output Video</td>
  </tr>
  <tr>
    <td>
      <iframe width="560" height="315"
      src="https://www.youtube.com/embed/It9Yuh0mTrs" frameborder="0"
      allowfullscreen></iframe>
    </td>
    <td>
      <iframe width="560" height="315"
      src="https://www.youtube.com/embed/ztwhL0oixws" frameborder="0"
      allowfullscreen></iframe>
    </td>
  </tr>
  <tr>
    <td>
      <iframe width="560" height="315"
      src="https://www.youtube.com/embed/7IRAKGoI0Ao" frameborder="0"
      allowfullscreen></iframe>
    </td>
    <td>
      <iframe width="560" height="315"
      src="https://www.youtube.com/embed/E2pUY2iaI08" frameborder="0"
      allowfullscreen></iframe>
    </td>
  </tr>
  <tr>
    <td>
      <iframe width="560" height="315"
      src="https://www.youtube.com/embed/m04VwI-Ca_k" frameborder="0"
      allowfullscreen></iframe>
    </td>
    <td>
      <iframe width="560" height="315"
      src="https://www.youtube.com/embed/oZWFxqzfj0A" frameborder="0"
      allowfullscreen></iframe>
    </td>
  </tr>
    <tr>
      <td>
	<iframe width="560" height="315" src="https://www.youtube.com/embed/vCb-oUCtXJY" frameborder="0" allowfullscreen></iframe>
    </td>
      <td>
	<iframe width="560" height="315" src="https://www.youtube.com/embed/zd8ytJvr19w" frameborder="0" allowfullscreen></iframe>
    </td>
  </tr>

</table>
<p>This group of results are less successful compared with the dataset
provided by the author. There are noticable bounce and
distortions. This group of videos were cpatured by a CMOS camera,
where rolling shutter effect will cause distortions when the camera
moves drastically and our subspace approximation does not hold in this
case.</p>

<h2>Limitations and failure case</h2>
<p>Although small portion of dynamic content is handled throught
  epipolar constraint, this algorithm fails where the majority of the
  video is non-rigid, see the following example:</p>
<table>
  <tr>
    <td>Input Video</td>
    <td>Output Video</td>
  <tr>
    <td>
      <iframe width="560" height="315" src="https://www.youtube.com/embed/6Jut36cb3Ko" frameborder="0" allowfullscreen></iframe>
    </td>
    <td>
      <iframe width="560" height="315" src="https://www.youtube.com/embed/uCDGCZGheVE" frameborder="0" allowfullscreen></iframe>
    </td>
  </tr>
</table>
<p>The algorithm also fails for drastic motion. See the example
  below (the output video are uncropped):</p>
<table>
  <tr>
    <td>Input Video</td>
    <td>Output Video</td>
  <tr>
    <td>
      <iframe width="560" height="315" src="https://www.youtube.com/embed/lxGAz8FZUe4" frameborder="0" allowfullscreen></iframe>
    </td>
    <td>
      <iframe width="560" height="315" src="https://www.youtube.com/embed/vXd1NmpavGg" frameborder="0" allowfullscreen></iframe>
    </td>
  </tr>
</table>
<p>The large bounding comes from two sources: 1. due to the drastic
  motion, the subspace approximation has large error even inside local
  window and 2. rolling shutter effect causes severe distortion to the
  video.</p>
</body>
</html>
